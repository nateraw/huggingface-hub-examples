{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speech-to-code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDQlvZ41TSF8NT2B7po1Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nateraw/huggingface-hub-examples/blob/main/speech_to_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCY116AnQT-2"
      },
      "source": [
        "%%capture\n",
        "! pip install speechbrain huggingface-hub gradio\n",
        "! pip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3-rp-b\n",
        "! apt install git-lfs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t7hdu-RQa7W"
      },
      "source": [
        "import gradio as gr\n",
        "from speechbrain.pretrained import EncoderDecoderASR\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GPTNeoForCausalLM,\n",
        ")\n",
        "from huggingface_hub import snapshot_download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz2eGZQ_hqao"
      },
      "source": [
        "## Load ASR Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwvAFh_uf5S7"
      },
      "source": [
        "asr_model = EncoderDecoderASR.from_hparams(\n",
        "    source=\"speechbrain/asr-transformer-transformerlm-librispeech\",\n",
        "    savedir=\"pretrained_models/asr-transformer-transformerlm-librispeech\",\n",
        "    run_opts={\"device\":\"cuda\"}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6_voUnwhsQh"
      },
      "source": [
        "## Load Python Code Completion Model\n",
        "\n",
        "This one takes quite a while to download, so maybe go get a coffee? â˜•"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZIdGejhUtaT"
      },
      "source": [
        "repo_dir = snapshot_download(\"NovelAI/genji-python-6B-split\")\n",
        "model = AutoModelForCausalLM.from_pretrained(repo_dir + \"/model\").half().eval().cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj540Lvdhutm"
      },
      "source": [
        "## Launch App"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PieP_1ImZVXE"
      },
      "source": [
        "def speech_to_text(file):\n",
        "    text = asr_model.transcribe_file(file.name)\n",
        "    text = f\"'''{text.lower()}'''\\n\"\n",
        "\n",
        "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        "    generated_tokens = model.generate(\n",
        "        tokens.long().cuda(),\n",
        "        use_cache=True,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        temperature=0.3,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.125,\n",
        "        min_length=1,\n",
        "        max_length=len(tokens[0]) + 400,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    last_tokens = generated_tokens[0][len(tokens[0]):]\n",
        "    generated_text = tokenizer.decode(last_tokens)\n",
        "    return text + generated_text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=speech_to_text,\n",
        "    inputs=gr.inputs.Audio(source=\"microphone\", type=\"file\", label=None),\n",
        "    outputs=\"text\"\n",
        ")\n",
        "iface.launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}